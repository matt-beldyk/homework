Basically, I iterate through the corpus one line at a time.  I keep track of which document I am in and add terms to my hash as I iterate (and an entry into each term's list showing which document held that term.)

I tokenized the documents by splitting on a regular expression that basically stated "one or more NOT(dashes or alpha-numerics)"  The semantics of Perl's split function meant that on lines with leading whitespace, I was finding the empty string (and thus I needed to prune it.)

For my output, I simply sorted the postings for each term, and then iterated over them to remove duplicates (if I cared about performance, I might have been inclined to use a different data structure than a list for these, but I would have to benchmark to determine which seemed to work the best. (In this case I optimized for developer free time.))

Short Answer:
Terms: 14480
Postings: 91009
Size of pointers internal to $index = 506158
Total Size of $index = 6224742
Size of output file = 495536

Interestingly enough, the internal perl data structure jumped to 6X the size of the initial corpus.  I assume this is due to perl being exceedingly generous about how it allocates space for strings and other structures.  This of course, does not work for a corpus of any large size; on my laptop I'd be into swap space at about a 0.5G corpus.

The output file format is 12X better than the internal perl data structure (which isn't sayning much,) but that is still problematic for anything large.

For a larger corpus, I'd have to do significant tuning of my data structure, possibly forcing the system to prune the ends of all my strings and ints (which are stored as strings.) I would also have to create a more efficient way to store this information (perhaps using some of the suggested optimizations from class.)



\documentclass{acm_proc_article-sp}
%\usepackage{graphicx}
%\usepackage[pdftex]{graphicx}
%\usepackage{subfig}
%\DeclareGraphicsExtensions{.pdf,.png,.jpg}



\makeatletter
\let\@copyrightspace\relax
\makeatother

\begin{document}

\title{Challenges and Oportunities of Augmented Reality on Cellphones}

\numberofauthors{1}
%
\author{
\alignauthor
Matthew Beldyk\\
\affaddr{University of Colorado}\\
\affaddr{Masters Comprehensive Exam}\\
\affaddr{Boulder, Colorado}\\
\email{beldyk@colorado.edu}
}
\maketitle
\begin{abstract}
ABstract goes here
\end{abstract}

\section{Introduction}
In recent years cellphones have become smaller, faster, more power efficient and are sold with a dizzying array of internal sensors.  This can be chalked up to the inevitability of Moore's law, but it means that what can be accomplished on a cellphone has expanded as well.  No longer are cellphones simple devices that a user can only use to  make a phone call or send an occasional text message. Modern cellphones are portable Internet terminals,; they are portable video players; they are a portable map that gives users information about any point on the map with a simple tap of the finger.  Modern cellphones come with larger high resolution screens, internal digital cameras, GPS receivers, accelerometers, digital compasses, and high speed network connectivity. Cellphones have replaced stand alone cameras for many users, and there are even apps that do nothing but turn the screen white so it can be used as a flashlight. This list of features, that all fit in a shirt pocket, would make the augmented reality developer from only 20 years ago green with envy.  As cellphones have become more ubiquitous, augmented reality applications have also begun to appear in the mainstream consciousness.  At first augmented reality applications were far and few between, in various computer science labs around the world, but as the decades wore on and cellphones became more powerful now these application are beginning to come as expected features in the average smartphone.  

Due to the advancement of cellphone technology there are vast opportunities to create portable augmented reality applications.  This paper will discuss some of the early history of augmented reality to put current developments in perspective and will then jump into a discussion of modern issues facing a developer for a cellphone platform (explained through the perspective of working applications and systems.)  This paper will discuss the plethora of sensors available for use on a cellphone and the problems inherent in these various sensors.  In addition, there will be a section discussing the large number of techniques available to provide location services to an application; both algorithms labs have developed in prototype form, as well as, fully implemented and tested systems that are available by default on cellphones will be discussed. There will also be a discussion of various computer vision techniques that have been proven to work within the constraints of mobile platforms.  Once the opportunities available on cellphones have been discussed, the constraints that still limit these systems will be discussed. There will also be an examination of several recent augmented reality applications on cellphones that are particularly novel, powerful, an extension of a classic application (but now in a much more portable form) or popular applications that are available to the general public through various means.

\subsection{What is augmented reality and an early history}

Augmented reality is what it's name describes; it is a system for adding additional information into a view of the real world \cite{mizell1994virtual} There is a wide spectrum that ranges from the real world (or the world viewed without any modification) to the completely artificial world, where everything is generated from a model. \cite{milgram1994taxonomy} The portion of this spectrum this paper addresses is somewhere in the middle of this spectrum.   Augmented reality describes systems where artificial virtual constructs are overlayed into a view of the real world; this is often done with a camera view of the real world (along with other sensors) and when the view is rendered, extra images are added into the final view.  It is also possible to simply project the artificial images directly over the real world, without reprojecting the view from a camera to the user.  In fact, one of the earliest virtual reality systems worked in exactly this way.  In the late 1960's  Ivan Sutherland created an early virtual reality system what measured the location of the user's gaze and would project a three dimensional world into their field of view.  There were two separate displays: one for each eye, and a system what would track the head as it moved.  The images were projected onto screens in front of each eye and would allow the user to see line art overlayed over the real world. \cite{Sutherland:1968:HTD:1476589.1476686}

The term "augmented reality" was created in 1992 by Tom Caudell and David Mizell when they were working for Boeing to create a system to assist people assembling the complex wiring harnesses for Boeing's equipment \cite{citeulike:6081386} \cite{carmigniani2011augmented}

In 1993, Fitzmaurice created an early prototype called Chameleon.  Chameleon appeared to be a palmtop computer that would allow the user to pan through a virtual world (or in the example they cite in the paper, browse a virtual 3d tree of data) by moving the device around in the real world.  In reality, this was a system that used a video camera on a SGI workstation to project the view into a small portable LCD tv that was connected to a 6D input sensor (x, y, z, and rotational information.)  Even though, the system was just a mockup of what one can see today in a modern cellphone, this mockup allowed much experimentation and learning about how people would expect a system like this to work.\cite{fitzmaurice1993situated}

Also in 1993, Loomis et al. ran several tests on users to discover their spatial navigation skills.  \cite{loomis1993nonvisual} During the process of these tests, they created a backpack based modular system that would assist the visually impared with navigational tasks.  This system consisted of several modules: a differential GPS and digital compass module, a GIS database of location information, and an auditory user interface.  The auditory based interface could operate in a number of different ways: conventional speech display through headphones or a virtual acoustic display through binaural headphones. \cite{loomis1994personal}

In 1995, Rekimoto and Nagao created a system called NaviCam, which could reasonably be considered to be an extension of the earlier Chameleon research. \cite{fitzmaurice1993situated}  NaviCam could work either as a palmtop device or with a head mounted display (although, both still relied on an external workstation to do most of the data processing.)  This new system used a small CCD camera attached to the display device and a system of colored tags to provide registration of object.  The authors present several examples using their system including tags on paintings in a museum, book identification in a library, and providing a personal calender when viewing a wall calendar. \cite{rekimoto1995world}
\section{Body}
\subsection{Location Services on cellphones}

An obvious problem with augmented reality is being able to find out where the system is and where it is pointing: registration.  A human may have a clear idea where they are located and what they are looking at, but this is much more difficult for a computer. \cite{szeliski2010computer}  There are a number of systems and sensors available to assist computers and cellphones with this task.  
Historically, mobile augmented reality systems have used external sensors because sensors internal to the mobile device did not have the precision required or simply did not exist.  In recent years, this trend has changed:  modern smart phones contain quite an array of sensors that are easily available to programmers.  

The first system that usually will come to mind if someone thinks about location is GPS (or Global Positioning System.)  GPS satellites transmit a signal that contains their orbit parameters, the time the signal was sent and information about the GPS satellite network.  GPS receivers listen for this information from several satellites to determine their location. \cite{hofmann1997global}  As good as GPS is, there are a number of pitfalls.  First, for a GPS receiver to work properly, it requires a line of sight view of at least four satellites; in an urban environment, this requirement will often make a GPS receiver either incredibly inaccurate or simply not operate at all.  Second, a GPS that is started without any prior knowledge of the satellite constellation, any vague idea of the current location, or what time it is (as is the case of what is called a "cold start") can take several minutes to recalculate this information from the GPS signal.  There is also the ever present problem of power usage in a mobile device, and activating GPS on a phone will quickly drain the battery.  \cite{djuknic2002geolocation}  For cellphones, there are other options that can ease these problems.  A-GPS is a system many cellular service providers provide to their customers.  A-GPS has two modes: MS-assisted GPS and MS-based GPS. MS-assisted GPS provides information about the current  satellite constellation to the cellphone over the usual terrestrial cellphone service to allow the GPS on the cellphone to better acquire a lock on satellites.  This information is calculated on the phone company's servers and includes a rough location of where the user is, ephemeris information, and estimated pseudorange information. MS-based GPS works by the cellphone service towers providing the same information as the assisted mode, but the software on the phone uses only that information to determine where it is located instead of using it as an augmentation to regular GPS. \cite{sun2005signal}

There are other techniques to determine location other than just GPS.  In fact, GPS doesn't work well inside, and thus there must be other ways to determine location.  Even if it did work inside, due to the margin of error inherent in GPS two stores next to each other could not be distinguished and thus different techniques must be utilized. It is possible to create a system of beacons using specialized hardware that transmit location information.  Cricket is one such example; it has a set of special beacons that transmit RF signals followed by a ultrasonic pulse.  The user (or listener device) will listen for a RF transmission then listen for an ultrasonic pulse; using this information, the user can then determine the nearest beacon and thus determine its location.  Although Cricket does work well, there are a couple problems with the design; the largest problem is the requirement for specialized equipment to be installed in all locations where the system might be used. \cite{priyantha2000cricket}  

Another possible technique is location fingerprinting; location fingerprinting is a technique that can be used to look at various features of a location to determine where the phone is located. Location fingerprinting can be done only using wifi signals; RADAR \cite{bahl2000radar} is such a system.  This system working by looking at the signal strength of wifi repeaters and nodes to determine its location. It is possible to work in two modes; the first is when only one signal is noticed, then the system just assumes that the user is nearby to that repeater.  The other mode is when multiple signals are seen; the system will triangulate based on signal strength to determine the location (after comparing the discovered signature and finding the k-nearest neighbors of pre-measured signatures).  This system is not as accurate as Cricket, but it does not require any specialized hardware and can be implemented far more easily due to the ready availability of wifi connection points.  \cite{bahl2000radar}  A problem with the RADAR system is due to the need to pre-measure many of the locations for access-points and create a large corpus of signatures. There has recently been some work to create a similar system that does not require this premeasurement.  EZ Localization works in a similar method to the earlier RADAR system, but relies on being able to get the occasional GPS lock when near a window or entrance to a building. The algorithm then uses advanced genetic search algorithms based around the physics of WiFi propagation to create the corpus of located signatures.  \cite{chintalapudi2010indoor}

It is also possible to use more features than just wifi signals to determin a location fingerprint. Azizyan et al. describe a technique that looks at ambient sound, wifi signals, colors and lighting to try and determine the location.   They discuss the problem in the context of the indoor localization problem where "place" is divided into discreet locations that have edge boundaries of importance far less that the margin of error from regular GPS.  The authors describe their system that they tested in a shopping mall to identify stores based on the previously listed charactaristics.  A potential problem of this approach is that the initial fingerprint data gathering is very work intensive and fingerprints change through out the day; the authors used a time window of two hours before a signature was considered invalid.  This short window makes manual data collection much more onerous. \cite{azizyan2009surroundsense}  Perhaps this onerous fingerprint collection issue could be solved in the future by having the server track fingerprint drift throughout the day as various users walk in and out of the stores.

\subsection{Sensors available on Cellphones}

Once the location issue is solved, one may need to determine the orientation of the cellular phone. Many modern cellphones are equipped with  accelerometers and digital compasses that can help with this problem.  The accelerometer can tell how the phone is tilted, and the compass can show the direction the phone is pointing.  Unfortunately, the compass will be inaccurate in the presence of large amounts of metal, electrical currents \cite{karpischek2009swisspeaks} and magnetic monopoles.  Cellphone accelerometers can be used to track motion of the phone in addition to simply giving the orientation of the phone.  In fact, there was been recent work for Android phones to attempt to make the phone recognize different activities that a user with the phone in their pocket may be doing.  \cite{kwapisz2010activity}  There are numerous uses for this new technology.  Kwapisz et al. suggest several including sending calls directly to voice mail while the user is exercising or playing "upbeat" music while the user is jogging,

Another use of the accelerometer in a cell phone is to assist in location.  The accelerometer gives information about how the user is moving, and with careful techniques, a cellphone can determine what path the user is taking as they walk. While the basic idea of navigation by tracking acceleration and inertial forces may not be new, \cite{farrell1999global} \cite{gustafsson2002particle} Constandache et al. presented a novel approach to this technique. They use A-GPS to get a rough location, then use the accelerometer data to find the path the user was taking, and use map data from Google Maps to find the closest match of the local roads and trails.   \cite{constandache2010towards}

\subsection{Visual techniques available for a cellphone}

Another method of registration that is available to cellphones is the two dimensional barcode similar to those that have historically been used by augmented reality systems since the 1990's.

In 1996, Rekimoto created Matrix. 2D barcode-like images are placed in the real world, which when viewed through the Matrix system would allow easy registration of location and orientation. Using this information, models are projected into Matrix's display.  For example, one could place the code for "dinosaur" on a table, and when viewed within the Matrix system, a dinosaur would appear to exist in three dimensions. Matrix set the bar for much of the future registration tasks; it only required a camera and some computer vision tequniques; it required no other ungainly external sensors to accomplish its world annotation.\cite{rekimoto2002matrix}  One can now see a more advanced version of 2D barcodes when one uses a QR code. QR Codes were created by DENSO WAVE INCORPORATED and software to read them is available on many modern cellphones. \cite{nojiri2004two}  A QR code is made up of several pieces.  The first is a set of squares on three of the corners to assist in identifying that a subsection of the image contains a code. There is also an additional pattern in the remaining corner to identify alignment of the code as well as a pattern to assist in timing.  Finally, there is the actual data contained within the code. \cite{ohbuchi2004barcode}

Over the years, these codes have proven popular with augmented reality researchers.  This is at least partially due to the existence of libraries that allow easy prototyping. A popular library is ARToolKit. \cite{kato1999marker}  ARToolKit has had bindings created for a number of languages and has been ported to a large number of platforms including Windows, OSX, Linux, and various mobile devices.  This toolkit offers a specific set of 2D "glyphs" (these 2D barcode markers) that one can simply print out with an ordinary printer and then easily use the library to recognize the glyphs and create a working program.  \cite{hornecker2005using}

An early example of the use of the ARToolKit framework is a project called SignPost. \cite{kalkusch2002structured}   This application would guide a user around an unknown building by giving various navigational hints based upon the registration of the ARToolKit glyphs.  The first iteration of SignPost was presented in 2002 and obviously had to use technology from that year. It was based on a 1 GHz laptop worn in a backpack and accordingly appears quite bulky.  Within a short period of time, a new client for this system was created.  This smaller version ran on a IPAQ with a CompactFlash camera, but still needed to offload more difficult processes onto an external workstation. \cite{wagner2003first}

Unfortunately, the natural world is not annotated with easily machine parseable glyphs, but this does not mean that there is nothing to attempt visual registration with.  With modern cellphones it is possible to use a myriad of computer vision techniques to do any sort of object recognition and registration that one might want.   For example, it is possible to render a view of a mountain skyline from the location of a user and then find the best match between the rendered skyline and the actual skyline seen by the camera to identify mountains and orientation of the camera. \cite{behringer2002registration}  Unfortunately, this process is still very processing intensive, even for modern phones, and much of the heavy processing still must be offloaded onto a server somewhere.  The Maramota project extends Behringer's earlier work and is able to run mostly on a cellular phone, but due to the size of the DEMs (digital elevation models) must do the render of the model skyline on a server. \cite{chippendale2009environmental}

It is also possible to create a system that visually recognizes buildings and can give users information about the building. Takacs et al. created a system that allows a user to take a picture with an GPS enabled camera phone and will annotate this photograph with information about the contents contained within.  The authors created a version of SIFT \cite{lowe1999object} and SURF \cite{bay2006surf} that runs on their test cellphones.  To create the custom algorithms for their cellphone, the authors had to improve the memory usage as well as speed up the computation of the features. To decrease the memory utilization, the authors only store one version of the image and translate between the original and the integral image as needed.  Their custom optimizations of the SURF algorithm run faster and use half the memory of the original algorithm.  Once this SIFT or SURF features are generated from the photograph, they are sent to a remote server and matched with other data from the same location.  Once a match is found, annotation information is returned to the cellphone and displayed.  The authors found that this interface of pointing a camera at an object and it being annotated was quite intuitive to users and that SURF performed the best in their situation.  \cite{takacs2008outdoors}

Another application of using visual techniques to identify entities in the real world is as a museum tour guide.  Museums are tightly controlled environments with controlled lighting and quite an array of sensors for monitoring the patrons. There two factors add up to the creation of the near perfect environment for visual techniques.   There have been a number of examples of prototype systems to give museum patrons information about the displays they are interested in.  An example of this is the system created for the Swiss National Museum by Bay et al.  This system is based around a tablet PC with an USB attached camera for object recgonition.  The authors used SURF features to find matches with objects between the real world and their database.  They had a fair amount of success with correct matches, but mentioned that there would be issues with searching the entire catalog of objects to try and find a match for each object the user is examining, so they suggest having location beacons in each room (perhaps based on Bluetooth) to narrow down the search space. \cite{bay2006interactive}  Another example of a similar system, is the work by Ruf et al. \cite{ruf2010mobile}  This work was done to show that using a clever client server architecture that the authors could create a system that would run on several models of unmodified mobile phones.

\subsection{Modern Augmented Reality Apps}
Now that cellphones that are orders of magnitude more powerful than desktop workstations from the early 1990's are available to the average phone user, there has been a bit of a renaissance in the mobile augmented reality world.  
There are a number of apps that have begun to appear that help users to identify locations and places.  For example, Karpischek et al. have created an application called SwissPeaks to help identify mountains.  SwissPeaks uses GPS, camera and the digital compass included with an iPhone 3GS to create a real time view of the world that would label selected mountain peaks in the Swiss Alps.  While on the surface this application does not appear to be any different than any other location layer application, this one has one critical difference: it calculates what peaks are actually visible to the user (ones that are not occluded by other peaks.)  The authors precompute what are the visible peaks from a set of test locations (several vallys in the Swiss Alps) and then return this information via webservices to the cellphone client when asked.  The authors note issues with the accuracy of the digital compass, but addressed this issue by allowing the user to manually adjust the compass in the interface.  \cite{karpischek2009swisspeaks}

Another example of a location information providing app is FIXME

Other examples of applications that provide overlays of information based on location and heading are Wikitude ( http://www.wikitude.org ) and Layar ( http://www.layar.com/ ).  Both of these applications are available via the Android App store and the iPhone App store for free.  These two applications utilize the camera, location services, digital compass and accelerometer of a phone.  The basic flow is to choose a layer of interest from a menu, then to switch to "camera" mode and the application will project the points in that layer into the screen where that particular location is located at.  Users can submit their own custom layers to display whatever information that they want, but the many top featured layers are all from various companies that are providing layers to help locate themselves or other companies (in the case of Yelp or Groupon.)  The fact that businesses are creating their own layers provides evidence that these are interesting applications that have a potential funding source, and quite frankly they are enjoyable to use.

Whenever I mention what I am writing about for this paper, and then explain what augmented reality is, people mention Google Sky Map.  Google Sky Map takes the location of the user, as well as the bearing information from the sensors of a phone to project a view of the stars into the screen of the phone making it easy for users to identify features in the night sky. This application will pan and move in sync with the user's movements of the phone, but in fact, Google Sky Map is not an augmented reality application.  There is one critical difference, the application does not take input from the camera and project its model of the sky onto that image. \cite{Ouilhet:2010:GSM:1851600.1851695}

An interesting recent augmented reality application was the recent advertizing campaign by Saatchi & Saatchi
for the Wellington Zoo.  This application worked by having a regular print advertizement in a newspaper that instructed users to send an SMS to a specific number.  Then the users would be redirected to a mobile page to download an application if their phone was compatable.  This application would run on the user's cellphone and when the user pointed their phone at the newspaper advertizement (which included a couple marker images) would display three dimentional models of animals on the user's cellphone, oriented by how the user way pointing the phone's camera.  \cite{henrysson2007bringing}
\subsection{Software Architecture Constraints}
Although modern smart phones are far more powerful than they have been previously, there are still numerous constraints upon development on these platforms.  A knee-jerk reaction to developing code for a smart phone it to simply port a current desktop application to the phone and simply change the interface so that it fits better on the smaller screen of the phone.  While in some cases this may work, there are problems with this approach.  Firstly, modern smart-phones do not have multiple fast processors to work with, nor do they have the same amount of memory available to use as a desktop application may have.  Secondly, there are limitations to the network resources available to a cell phone; network bandwidth still costs more on a phone than it does on a regular desktop workstation.  \cite{wagner2009making}  

There are a number of ways to counter these issues, in many cases the hard processing work can be offloaded onto an external server that does not suffer from the same power and processor constraints that a portable cellphone will. These services will often be connected to the cellphone via various webservices. While in many cases an external Webservice is a good option, in others this simply is not the case. In some cases tight coupling with the internal sensors of the device is required.  For example it does not make sense to offload processes to external servers when the network latency is larger than the processor time on the actual device.  Also, for privacy reasons, it might be undesirable to pass certain specific data back to an external server. When processing is required to run upon an actual mobile device, the algorithms and data structures required for smaller mobile devices can be quite different from the structures allowed on larger desktop systems.  \cite{wagner2009making} Another solution to ease the dearth of processor power is to use libraries specifically written for smaller mobile devices.  These libraries will often be written by developers with expertise knowledge about the underlying platform, and they will be able to utilize and special features or optimizations that might be available on that particular platform.  

\subsection{Display Constraints}
As cellphones have become smaller and more powerful, the same can be said for their displays.  Even the largest cellphones today only have displays that are a couple inches wide, and that is only for the largest smart phones.  The more average size of a cellphone screen is even smaller, with a width of only an inch or two.  Even if these phones were to have the resolution of a regular computer display, the screens would be too small to allow a user to read anything with fonts that small.

There are a number of problems that arise when a system attempts to render too many points in a single display.  With too many points the display becomes unreadable and a user is simply not able to discover any information from the system.  Another issue is performance, the more points to render into the screen of the phone, the slower the interface will become.  There are a number of ways to counter this problem, the first that is always referred to in papers is to simply limit rendered points to those nearby.  \cite{pombinholocation} \cite{FIMEMORE} The next teqnique is to only render what is visible, or location in the direction the phone camera is pointing.  There are a number of ways to determine the point of view and what is visible such as pruning the list based on angle of view or relying on more high level techniques provided by the libraries available on the phone such as OpenGL or other libraries that have been highly optimized for the phone.  

Once the system knows what is visible, there are still a number of ways to improve what is displayed.  Pombinho et al. describe several approaches to remove "over clutter" for their application in their user tests.  Pombinho et al. created an application for a 200Mhz Pocket Pc with less than 200MB total memory, with a screen resolution of 320x240 running Windows Mobile 5.0 with a database running on a server to provide location data.  First they describe a system that will determine a "degree of interest" for a particular point. This is a function that will take the weights of what is "important" to the user and weight it against the distance to that point; this work is an extension of the earlier work by Carmo et al. \cite{carmo2008movisys}  Their next step was a system of aggregation operators, where multiple similar nearby icons could be joined into a single icon that would convey that there were multiple points of note in that particular location.  After this, the authors describe their system of joining nearby dissimilar icons into an icon that displayed information about all of the nearby joined icons.  The authors also tested algorithms for displacing icons that would overlap and adding a visual pointer back to where the actual location of the icon should be.  The authors found that users could not agree on what reasonable icon sizes were; there was quite a spread of what their test users liked.  The authors, however, did discover that the aggregation and typification operators were found to be quite helpful to the users to create a more usable and navigable interface. The authors also discovered issues with the "degree of interest" algorithm.  Because it did not act as a binary operator, when users would search for "Italian food" and there was none to be located on the map, the map would still display various restaurants because their features could be considered close to "Italian food" and users immediately assumed that anything displayed would be Italian due to their previous experience with search systems.  \cite{pombinho2009evaluation}

There are also ways to display information other than placing icons on a map or overlaying onto a camera view.  The application "Point to Discover" allows a user to point a modified cellphone (to add GPS functionality, an accelerometer and a digital compass) at a location in the world and will display what is located in that direction.  This application takes the novel approach of using an model of buildings as block-like objects to determine what is actually visible.  Using the information of what can actually be seen, the app displays a simple list of what is visible. While a simple list may not incite as much excitement as an overlay onto a camera output of the real world, this app does illustrate that it is possible to do complex augmented reality applications on older cellphones, and all the modifications the authors had to do to their phone are easily available in many modern cellphones today. \cite{simonpoint}

An even more radical way to "display" information is to not display it at all.  An application can have an audio only interface that adds a location tagged audio track into the headphones of the user.  An example of this is "Riot! 1831." "Riot! 1831" is an application that runs on an iPAQ computer with an external GPS receiver to give locational data. This application would play sounds to users to lead them through a tour of Queens Square, Bristol England, the sounds played were an audio drama of a riot that took place in 1831.  When a user would enter an area of interest, the local sound track would begin to play, and when they left, the soundtrack would end. \cite{reid2004riot} \cite{reid2005parallel}   Even as early as 1993, were applications like this available, although in 1993 the application needed to be housed in much larger devices than an iPAQ.  Loomis et al. created a backpack wearable system that would help blind users navigate by auditory clues. \cite{loomis1994personal}

wikitude, layar
some other modern apps to help identify places and things
talk about the ar toolkit provided by nintendo ds?

\section{Conclusions}
blah blah blah, things are getting better

\bibliographystyle{abbrv}
\bibliography{ar_cell_paper}

\end{document}
